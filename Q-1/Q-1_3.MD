- Are the results as expected? Why or why not?
   - Training Time: The results are as expected because I expect the larger models to take more time to train with the exception of VGG 16 with pretrained Conv Layers, which takes lesser time.
   - The number of model parameters and the model size also follow the expected trend because more the hidden neurons, more the number of weights.
   - The results in accuracy and loss are not as expected due to the model getting stuck in some local minima. In general, I expected more layers -> more accuracy. 
- Does data augmentation help? Why or why not?
    - Data augmentation increases the training time.
    - Size of the model remain same.
    - Leads to slight decrease in accuracy.
    - In large datasets, it may be helpful to prevent overfitting, and accurate predictions on real life data. 
- Does it matter how many epochs you fine tune the model? Why or why not?
    - Yes, it matters.
    - In "earlier" epochs, we see the testing and training accuracies increase while loss decreases.
    - After some time, the training loss decreases while the testing loss strats increasing.
    - This is when we can observe the model to begin overfitting.
    - A good number of epochs is thus required.
- Are there any particular images that the model is confused about? Why or why not?
   - In our dataset of Honeybee and Apples, we cannot see such images generally because the classes look very different and there are scarcely any points that can be confused.